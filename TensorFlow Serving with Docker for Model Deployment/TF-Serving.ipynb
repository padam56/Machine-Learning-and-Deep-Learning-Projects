{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"> Deploy Models with TensorFlow Serving and Docker</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "ZQhSNW-CZUZh",
    "outputId": "0a0dd94e-b78f-4ffb-c819-5eae6efb6255"
   },
   "outputs": [],
   "source": [
    "#%%writefile -a train.py\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id,ProductId,UserId,ProfileName,HelpfulnessNumerator,HelpfulnessDenominator,Score,Time,Summary,Text\n",
      "184502,B001BCVY4W,A1JMR1N9NBYJ1X,Mad Ethyl Flint,0,0,4,1228176000,Doesn't look like catfood!,\"When you first open the can, it looks like something you would eat.  And no catfood smell! Nice sized chunks of chicken and vegetables in a lot of gravy.<br /><br />That being said, Ms Casiopia lapped up all the gravy and left the rest.  This however is not the product's fault as she has done this before with other catfoods<br /><br />I would have given it 5 stars, but since I won't be purchasing it, I gave it 4.  If your cat will eat chunks and vegetables, this product is for you.<br /><br />I have donated the remainder of the package to a less fortunate friend.<br /><br />Thank you.\"\n"
     ]
    }
   ],
   "source": [
    "#Souce: https://www.kaggle.com/snap/amazon-fine-food-reviews/data\n",
    "!head -n 2 train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile -a train.py\n",
    "\n",
    "def load_dataset(file_path, num_samples):\n",
    "    df = pd.read_csv(file_path, usecols=[6, 9], nrows=num_samples)\n",
    "    df.columns = ['rating', 'title']\n",
    "\n",
    "    text = df['title'].tolist()\n",
    "    text = [str(t).encode('ascii', 'replace') for t in text]\n",
    "    text = np.array(text, dtype=object)[:] # : is for returning all the entries\n",
    "    #creating 1D array of strings\n",
    "    labels = df['rating'].tolist()\n",
    "    labels = [1 if i>=4 else 0 if i==3 else -1 for i in labels] #3 Classes instead of 5 #1 is for positive review\n",
    "    labels = np.array(pd.get_dummies(labels), dtype=int)[:] #one-hot encoding\n",
    "\n",
    "    return labels, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_labels, tmp_text = load_dataset('train.csv', 100) #loading only small no. of samples\n",
    "tmp_text.shape\n",
    "#tmp_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Classification Model using Keras and TF Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP Pre-processing steps:\n",
    "\n",
    "-Must Do:\n",
    "\n",
    "    Noise removal\n",
    "    Lowercasing (can be task dependent in some cases)\n",
    "-Should Do:\n",
    "\n",
    "    Simple normalization — (e.g. standardize near identical words)\n",
    "-Task Dependent:\n",
    "\n",
    "    Advanced normalization (e.g. addressing out-of-vocabulary words)\n",
    "    Stop-word removal\n",
    "    Stemming / lemmatization\n",
    "    Text enrichment / augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for: tokenization vs lemmatization vs stemming\n",
    "\n",
    "Building a clasification model that uses a pre-trained text embedding layer and accepting input as raw text and does the pre-processing itself by splitting the text on spaces. Don't need much of a additional pre-processing like tokenization/stemming/removing stopwords and so on because the module that we use here from TF Hub does all that for us. All it needs is a batch of sentences in a 1D tensor strings as an input.\n",
    "\n",
    "TF Hub is a library for publication, discovery and consumption of reusable parts of ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile -a train.py\n",
    "##https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\n",
    "##https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1\n",
    "def get_model():\n",
    "    hub_layer = hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\", output_shape=[50], \n",
    "                           input_shape=[], dtype=tf.string, name='input', trainable=False) #weights are not updated during propagation as we want to use word embeddings that have been already learnt and then just add a tiny fully connected layer and add a classification head on the top of that.\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(hub_layer)\n",
    "    model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(3, activation='softmax', name='output'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='Adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 50), dtype=float32, numpy=\n",
       "array([[ 0.05650096,  0.2567145 ,  0.24404189,  0.14395264, -0.05569138,\n",
       "        -0.10513686,  0.09544804,  0.3080969 , -0.218672  , -0.03048538,\n",
       "        -0.19036277,  0.01005417,  0.11541115, -0.14860378,  0.03914931,\n",
       "        -0.2561884 , -0.15442336,  0.12836292,  0.0469152 , -0.1500514 ,\n",
       "        -0.13068351, -0.01958708,  0.09192695,  0.1208052 , -0.12291992,\n",
       "        -0.04548305, -0.3679261 ,  0.05125156,  0.09797382, -0.10217863,\n",
       "        -0.1965521 ,  0.15523128, -0.05881735, -0.16426983,  0.06646369,\n",
       "         0.05789638,  0.15421619, -0.24014738,  0.11075415, -0.10756174,\n",
       "        -0.01679449, -0.01877424,  0.18602087,  0.2623015 , -0.3829217 ,\n",
       "        -0.34895867, -0.0868978 ,  0.02295742,  0.03787762, -0.02646483],\n",
       "       [-0.01533648,  0.2517981 ,  0.15771465,  0.10011643, -0.03027005,\n",
       "        -0.09655963,  0.10035348, -0.13405894, -0.13515756,  0.15999079,\n",
       "        -0.0257801 ,  0.01482286,  0.17336626,  0.02416893, -0.02589497,\n",
       "        -0.2256546 , -0.10834836,  0.05091727, -0.01329861, -0.1124052 ,\n",
       "        -0.04385714, -0.16535808,  0.07700986, -0.04862161,  0.0580256 ,\n",
       "         0.06278763, -0.10784025,  0.11745881,  0.07221924, -0.15103991,\n",
       "        -0.07155664, -0.00210649, -0.00439631, -0.24547555, -0.16631113,\n",
       "        -0.20567422, -0.05564746, -0.14419016, -0.12905043, -0.056013  ,\n",
       "        -0.10155825, -0.18731159,  0.1180155 ,  0.277938  , -0.02531946,\n",
       "         0.02398384, -0.12459337, -0.03263708,  0.00738647, -0.07372268]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\")\n",
    "embeddings = embed([\"this is a test\", \"look at the embeddings\"])\n",
    "embeddings #embedding vectors of length 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile -a train.py\n",
    "\n",
    "def train(EPOCHS=5, BATCH_SIZE=32, TRAIN_FILE='train.csv', VAL_FILE='test.csv'):\n",
    "    WORKING_DIR = os.getcwd() #use to specify model checkpoint path\n",
    "    print(\"Loading training/validation data ...\")\n",
    "    y_train, x_train = load_dataset(TRAIN_FILE, num_samples=100000)\n",
    "    y_val, x_val = load_dataset(VAL_FILE, num_samples=10000)\n",
    "\n",
    "    print(\"Training the model ...\")\n",
    "    model = get_model()\n",
    "    model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1,\n",
    "              validation_data=(x_val, y_val),\n",
    "              callbacks=[tf.keras.callbacks.ModelCheckpoint(os.path.join(WORKING_DIR,\n",
    "                                                                         'model_checkpoint'),\n",
    "                                                            monitor='val_loss', verbose=1,\n",
    "                                                            save_best_only=True,\n",
    "                                                            save_weights_only=False,\n",
    "                                                            mode='auto')]) #set the mode (val loss) to min or set as auto\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Export Model as Protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training/validation data ...\n",
      "Training the model ...\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (KerasLayer)           (None, 50)                48190600  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                816       \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 48,191,467\n",
      "Trainable params: 867\n",
      "Non-trainable params: 48,190,600\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.5963 - accuracy: 0.7845 - val_loss: 0.5704 - val_accuracy: 0.7881\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.57044, saving model to C:\\Users\\KIIT\\Everything\\Deploy-Deep-Learning-Models-TF-Serving-Docker\\model_checkpoint\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\KIIT\\Everything\\Deploy-Deep-Learning-Models-TF-Serving-Docker\\model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\KIIT\\Everything\\Deploy-Deep-Learning-Models-TF-Serving-Docker\\model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "3125/3125 [==============================] - 9s 3ms/step - loss: 0.5668 - accuracy: 0.7905 - val_loss: 0.5655 - val_accuracy: 0.7876\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.57044 to 0.56548, saving model to C:\\Users\\KIIT\\Everything\\Deploy-Deep-Learning-Models-TF-Serving-Docker\\model_checkpoint\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\KIIT\\Everything\\Deploy-Deep-Learning-Models-TF-Serving-Docker\\model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\KIIT\\Everything\\Deploy-Deep-Learning-Models-TF-Serving-Docker\\model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "3125/3125 [==============================] - 8s 2ms/step - loss: 0.5618 - accuracy: 0.7916 - val_loss: 0.5622 - val_accuracy: 0.7892\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.56548 to 0.56219, saving model to C:\\Users\\KIIT\\Everything\\Deploy-Deep-Learning-Models-TF-Serving-Docker\\model_checkpoint\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\KIIT\\Everything\\Deploy-Deep-Learning-Models-TF-Serving-Docker\\model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\KIIT\\Everything\\Deploy-Deep-Learning-Models-TF-Serving-Docker\\model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "3125/3125 [==============================] - 10s 3ms/step - loss: 0.5589 - accuracy: 0.7925 - val_loss: 0.5605 - val_accuracy: 0.7913\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.56219 to 0.56055, saving model to C:\\Users\\KIIT\\Everything\\Deploy-Deep-Learning-Models-TF-Serving-Docker\\model_checkpoint\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\KIIT\\Everything\\Deploy-Deep-Learning-Models-TF-Serving-Docker\\model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\KIIT\\Everything\\Deploy-Deep-Learning-Models-TF-Serving-Docker\\model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "3125/3125 [==============================] - 8s 3ms/step - loss: 0.5569 - accuracy: 0.7934 - val_loss: 0.5621 - val_accuracy: 0.7891\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.56055\n",
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: amazon_review/1624968631\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: amazon_review/1624968631\\assets\n"
     ]
    }
   ],
   "source": [
    "#%%writefile -a train.py\n",
    "\n",
    "def export_model(model, base_path=\"amazon_review/\"):\n",
    "    path = os.path.join(base_path, str(int(time.time())))\n",
    "    tf.saved_model.save(model, path)\n",
    "\n",
    "\n",
    "if __name__== '__main__':\n",
    "    model = train()\n",
    "    export_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negative Review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47671187, 0.07938445, 0.44390374]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = \"horrible book, waste of time\"\n",
    "model.predict([test_sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positive Review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02953439, 0.03649703, 0.9339686 ]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = \"Awesome product.\"\n",
    "model.predict([test_sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model is saved and exported to the subfolder of amazon_review folder, where the subfolders name is just the timestamp for when the model was exported. So, any future models we train will also be dropped or exported into the amazon_review parent folder meaning that there will be new timestamp folders within this directory here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Serving with Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`docker pull tensorflow/serving`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exposing two ports 8500 & 8501\n",
    "\n",
    "`docker run -p 8500:8500 \\ For gRPC clients\n",
    "            -p 8501:8501 \\ For rest end points\n",
    "            --mount type=bind,\\\n",
    "            source=amazon_review/,\\\n",
    "            target=/models/amazon_review \\\n",
    "            -e MODEL_NAME=amazon_review \\\n",
    "            -t tensorflow/serving`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup a REST Client to Perform Model Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Model Prediction\n",
    "\n",
    "##### Support for gRPC and REST\n",
    "\n",
    "- TensorFlow Serving supports\n",
    "    - Remote Procedure Protocal (gRPC)\n",
    "    - Representational State Transfer (REST)\n",
    "- Consistent API structures\n",
    "- Server supports both standards simultaneously\n",
    "- Default ports:\n",
    "    - RPC: 8500\n",
    "    - REST: 8501"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions via REST\n",
    "\n",
    "- Standard HTTP POST requests\n",
    "- Response is a JSON body with the prediction\n",
    "- Request from the default or specific model\n",
    "\n",
    "Default URI scheme:\n",
    "\n",
    "`http://{HOST}:{PORT}/v1/models/{MODEL_NAME}`\n",
    "\n",
    "Specific model versions:\n",
    "\n",
    "`http://{HOST}:{PORT}/v1/models/{MODEL_NAME}[/versions/{MODEL_VERSION}]:predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3NND75SMZUsP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tf_serving_rest_client.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tf_serving_rest_client.py\n",
    "import json\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "def get_rest_url(model_name, host='127.0.0.1', port='8501', verb='predict', version=None):\n",
    "    \"\"\" generate the URL path\"\"\"\n",
    "    url = \"http://{host}:{port}/v1/models/{model_name}\".format(host=host, port=port, model_name=model_name)\n",
    "    if version:\n",
    "        url += 'versions/{version}'.format(version=version)\n",
    "    url += ':{verb}'.format(verb=verb)\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_model_prediction(model_input, model_name='amazon_review', signature_name='serving_default'):\n",
    "    \"\"\" no error handling at all, just poc\"\"\"\n",
    "\n",
    "    url = get_rest_url(model_name)\n",
    "    #In the row format, inputs are keyed to instances key in the JSON request.\n",
    "    #When there is only one named input, specify the value of instances key to be the value of the input:\n",
    "    data = {\"instances\": [model_input]}\n",
    "    \n",
    "    rv = requests.post(url, data=json.dumps(data))\n",
    "    if rv.status_code != requests.codes.ok:\n",
    "        rv.raise_for_status()\n",
    "    \n",
    "    return rv.json()['predictions']\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    print(\"\\nGenerate REST url ...\")\n",
    "    url = get_rest_url(model_name='amazon_review')\n",
    "    print(url)\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\nEnter an Amazon review [:q for Quit]\")\n",
    "        if sys.version_info[0] <= 3:\n",
    "            sentence = input()\n",
    "        if sentence == ':q':\n",
    "            break\n",
    "        model_input = sentence\n",
    "        model_prediction = get_model_prediction(model_input)\n",
    "        print(\"The model predicted ...\")\n",
    "        print(model_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup a gRPC Client to Perform Model Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified from [https://github.com/tensorflow/serving/blob/master/tensorflow_serving/example/mnist_client.py](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/example/mnist_client.py#L152)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions via gRPC\n",
    "\n",
    "More sophisticated client-server connections\n",
    "\n",
    "- Prediction data has to be converted to the Protobuf format\n",
    "- Request types have designated types, e.g. float, int, bytes\n",
    "- Payloads need to be converted to base64\n",
    "- Connect to the server via gRPC stubs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gRPC vs REST: When to use which API standard\n",
    "\n",
    "- Rest is easy to implement and debug\n",
    "- RPC is more network efficient, smaller payloads\n",
    "- RPC can provide much faster inferences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jKJVOjDlZUvc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tf_serving_grpc_client.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tf_serving_grpc_client.py\n",
    "import sys\n",
    "import grpc\n",
    "from grpc.beta import implementations\n",
    "import tensorflow as tf\n",
    "from tensorflow_serving.apis import predict_pb2\n",
    "from tensorflow_serving.apis import prediction_service_pb2, get_model_metadata_pb2\n",
    "from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
    "\n",
    "\n",
    "def get_stub(host='127.0.0.1', port='8500'):\n",
    "    channel = grpc.insecure_channel('127.0.0.1:8500') \n",
    "    stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "    return stub\n",
    "\n",
    "\n",
    "def get_model_prediction(model_input, stub, model_name='amazon_review', signature_name='serving_default'):\n",
    "    \"\"\" no error handling at all, just poc\"\"\"\n",
    "    request = predict_pb2.PredictRequest()\n",
    "    request.model_spec.name = model_name\n",
    "    request.model_spec.signature_name = signature_name\n",
    "    request.inputs['input_input'].CopyFrom(tf.make_tensor_proto(model_input))\n",
    "    response = stub.Predict.future(request, 5.0)  # 5 seconds\n",
    "    return response.result().outputs[\"output\"].float_val\n",
    "\n",
    "\n",
    "def get_model_version(model_name, stub):\n",
    "    request = get_model_metadata_pb2.GetModelMetadataRequest()\n",
    "    request.model_spec.name = 'amazon_review'\n",
    "    request.metadata_field.append(\"signature_def\")\n",
    "    response = stub.GetModelMetadata(request, 10)\n",
    "    # signature of loaded model is available here: response.metadata['signature_def']\n",
    "    return response.model_spec.version.value\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"\\nCreate RPC connection ...\")\n",
    "    stub = get_stub()\n",
    "    while True:\n",
    "        print(\"\\nEnter an Amazon review [:q for Quit]\")\n",
    "        if sys.version_info[0] <= 3:\n",
    "            sentence = raw_input() if sys.version_info[0] < 3 else input()\n",
    "        if sentence == ':q':\n",
    "            break\n",
    "        model_input = [sentence]\n",
    "        model_prediction = get_model_prediction(model_input, stub)\n",
    "        print(\"The model predicted ...\")\n",
    "        print(model_prediction)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "deploy-TF-Serving.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
